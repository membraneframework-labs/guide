# Speech to text

## Installation

```elixir
File.cd(__DIR__)
Logger.configure(level: :error)

Mix.install(
  [
    {:bumblebee, "~> 0.3.0"},
    {:exla, "~> 0.2"},
    {:membrane_core, "~> 0.12.7"},
    {:membrane_ffmpeg_swresample_plugin, "~> 0.17.0"},
    {:membrane_raw_audio_parser_plugin, "~> 0.2.0"},
    {:membrane_portaudio_plugin, "~>0.16.0"}
  ],
  config: [
    nx: [default_backend: EXLA.Backend]
  ]
)
```

You need to have `FFmpeg` installed. For installation details take a look [here](https://www.ffmpeg.org/).

Furthermore, there is a possibility, that the `XLA` backend  (used by `Nx` to improve the performance tensor operations) is not automatically installed - in such a case follow the instruction [here](https://hexdocs.pm/exla/EXLA.html#module-xla-binaries).

## Element performing speech to text

We need to write our own sink that will perform speech to text and display the resulting transcription. Let's call this element `SpeechToText`.

For the purpose of performing the transcription the element will use the Open AI's [Whisper](https://openai.com/research/whisper) model. 
It can be easily loaded and the used with a "little" help from [Bumblebee](https://github.com/elixir-nx/bumblebee).

Whipser model requires the input audio samples to be in `f32le` format, which means, that they are represented as a floating numbers written on 32 bits, with little endian bytes order. The required input sample rate is 16 kHz as well as only single channel samples are allowed.

### Initialization

In the initialization process, we load the `Whisper` model, along with the featurizer, tokenizer and the generation configuration. Then we create a speech to text serving, and indicate that we want to use `EXLA` backend for the tensor operations.

### Buffers handling

Once a buffer arrives, we store its payload along all already seen audio samples in the element's state.

The audio needs to be split into chunks and the transcription is performed on each of these chunks.
With the `@chunk_duration` attribute we can specify how long the chunk should be.

In case there are enough samples already stored in the element's state and no transcription occurs at a time, the transcription is launched as an asynchronous task. Once its results are available, a message of form `{:transcribed, transcription}` is sent to the element.

We reflect these requirements in the definition of the accepted input stream format: `Membrane.RawAudio`.

```elixir
alias Membrane.RawAudio

defmodule SpeechToText do
  use Membrane.Sink

  @sample_rate 16_000
  # sample format is `:f32le`, so each format is written on 32 bits = 4 bytes
  @bytes_per_sample 4
  @channels 1
  # seconds
  @chunk_duration 5
  @chunk_size @sample_rate * @bytes_per_sample * @channels * @chunk_duration

  def_input_pad(:input,
    accepted_format: %RawAudio{
      channels: @channels,
      sample_rate: @sample_rate,
      sample_format: :f32le
    },
    demand_mode: :auto
  )

  @impl true
  def handle_init(_ctx, _opts) do
    {:ok, whisper} = Bumblebee.load_model({:hf, "openai/whisper-tiny"})
    {:ok, featurizer} = Bumblebee.load_featurizer({:hf, "openai/whisper-tiny"})
    {:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "openai/whisper-tiny"})
    {:ok, generation_config} = Bumblebee.load_generation_config({:hf, "openai/whisper-tiny"})

    serving =
      Bumblebee.Audio.speech_to_text(whisper, featurizer, tokenizer, generation_config,
        defn_options: [compiler: EXLA]
      )

    state = %{serving: serving, samples: <<>>, transcribing?: false}
    {[], state}
  end

  @impl true
  def handle_write(:input, buffer, _ctx, state) do
    all_samples = state.samples <> buffer.payload

    if byte_size(all_samples) > @chunk_size and not state.transcribing? do
      transcribe(all_samples, state.serving)
      {[], %{state | samples: <<>>, transcribing?: true}}
    else
      {[], %{state | samples: all_samples}}
    end
  end

  defp transcribe(data, serving) do
    send_to = self()

    Task.async(fn ->
      model_input = Nx.from_binary(data, :f32)
      results = Nx.Serving.run(serving, model_input)
      transcription = Enum.map(results.results, & &1.text) |> Enum.join()
      send(send_to, {:transcribed, transcription})
    end)
  end

  @impl true
  def handle_info({:transcribed, transcription}, _ctx, state) do
    IO.inspect(transcription, label: "Transcription")
    state = %{state | transcribing?: false}
    {[], state}
  end

  @impl true
  def handle_info(_other, _ctx, state) do
    {[], state}
  end
end
```

## Pipeline structure definition

The pipeline consists of three elements:

* the `Membrane.PortAudio.Source` - responsible for fetching the audio input from your microphone
* the `Membrane.FFmpeg.SWResampleConverter` - responsible for performing the sample rate conversion to the desired 16 kHz as well as the sample format conversion
* the `SpeechToText` sink we have previously created

```elixir
alias Membrane.FFmpeg

import Membrane.ChildrenSpec

structure =
  child(:microphone, Membrane.PortAudio.Source)
  |> child(:converter, %FFmpeg.SWResample.Converter{
    output_stream_format: %RawAudio{channels: 1, sample_format: :f32le, sample_rate: 16_000}
  })
  |> child(:speech_to_text, SpeechToText)

:ok
```

## Running of the pipeline

```elixir
alias Membrane.RCPipeline

pipeline = RCPipeline.start!()
```

Finally we can commision `spec` action execution with the previously created `structure`:

```elixir
RCPipeline.exec_actions(pipeline, spec: structure)
```

The prints with the transcription should start appearing below the last cell you have evaluated.
Try saying something to the microphone in english and the transcription of your words should appear.

You can terminate the pipeline with the following code:

```elixir
RCPipeline.terminate(pipeline)
```
