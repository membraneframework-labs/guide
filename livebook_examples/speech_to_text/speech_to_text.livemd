# Speech to text

## Introduction

This livebook example shows how to perform a real-time speech-to-text conversion with the use of the [Membrane Framework](https://github.com/membraneframework) and the [Bumblebee](https://github.com/elixir-nx/bumblebee).

You will see how to fetch the audio from your microphone, perform preprocessing and create your own Membrane element that runs an AI speech-to-text conversion model.

## Installation

```elixir
File.cd(__DIR__)
Logger.configure(level: :error)

Mix.install(
  [
    {:bumblebee, "~> 0.3.0"},
    {:exla, "~> 0.2"},
    {:membrane_core, "~> 0.12.7"},
    {:membrane_ffmpeg_swresample_plugin, "~> 0.17.0"},
    {:membrane_raw_audio_parser_plugin, "~> 0.2.0"},
    {:membrane_portaudio_plugin, "~>0.16.0"}
  ],
  config: [
    nx: [default_backend: EXLA.Backend]
  ]
)
```

You need to have `FFmpeg` installed. For installation details take a look [here](https://www.ffmpeg.org/).

Furthermore, there is a possibility, that the `XLA` backend  (used by `Nx` to improve the performance tensor operations) is not automatically installed - in such a case follow the instruction [here](https://hexdocs.pm/exla/EXLA.html#module-xla-binaries).

## Element performing a speech to text

We need to write a custom sink that will perform speech-to-text and display the resulting transcription. Let's call this element `SpeechToText`.

To perform the transcription the element will use the Open AI's [Whisper](https://openai.com/research/whisper) model. 
It can be easily loaded and used with a "little" help from [Bumblebee](https://github.com/elixir-nx/bumblebee).

The Whipser model requires the input audio samples to be in `f32le` format, which means, that they are represented as floating numbers written on 32 bits, with little endian bytes order. The required input sample rate is 16 kHz, as well as only single-channel samples, are allowed.

### Initialization

In the initialization process, we load the `Whisper` model, along with the featurizer, tokenizer, and generation configuration. Then we create a speech-to-text serving and indicate that we want to use `EXLA` backend for the tensor operations.

### Buffers handling

Once a buffer arrives, we store its payload along with all already-seen audio samples in the element's state.

The audio needs to be split into chunks and the transcription is performed on each of these chunks.
With the `@chunk_duration` attribute we can specify how long the chunk should be.

In case there are enough samples already stored in the element's state and no transcription occurs at a time, the transcription is launched as an asynchronous task. Once its results are available, a message of the form `{:transcribed, transcription}` is sent to the element.

We reflect these requirements in the definition of the accepted input stream format: `Membrane.RawAudio`.

```elixir
alias Membrane.RawAudio

defmodule SpeechToText do
  use Membrane.Sink

  require Membrane.Logger

  @sample_rate 16_000
  # sample format is `:f32le`, so each format is written on 32 bits = 4 bytes
  @bytes_per_sample 4
  @channels 1
  # seconds
  @chunk_duration 5
  @chunk_size @sample_rate * @bytes_per_sample * @channels * @chunk_duration

  def_input_pad(:input,
    accepted_format: %RawAudio{
      channels: @channels,
      sample_rate: @sample_rate,
      sample_format: :f32le
    },
    flow_control: :auto
  )

  @impl true
  def handle_setup(_ctx, _initial_state) do
    {:ok, whisper} = Bumblebee.load_model({:hf, "openai/whisper-tiny"})
    {:ok, featurizer} = Bumblebee.load_featurizer({:hf, "openai/whisper-tiny"})
    {:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "openai/whisper-tiny"})
    {:ok, generation_config} = Bumblebee.load_generation_config({:hf, "openai/whisper-tiny"})

    serving =
      Bumblebee.Audio.speech_to_text(whisper, featurizer, tokenizer, generation_config,
        defn_options: [compiler: EXLA]
      )

    state = %{serving: serving, samples: <<>>, transcribing?: false}
    {[], state}
  end

  @impl true
  def handle_buffer(:input, buffer, _ctx, state) do
    all_samples = state.samples <> buffer.payload

    if byte_size(all_samples) > @chunk_size and not state.transcribing? do
      transcribe(all_samples, state.serving)
      {[], %{state | samples: <<>>, transcribing?: true}}
    else
      {[], %{state | samples: all_samples}}
    end
  end

  defp transcribe(data, serving) do
    send_to = self()

    Task.async(fn ->
      model_input = Nx.from_binary(data, :f32)
      results = Nx.Serving.run(serving, model_input)
      transcription = Enum.map(results.results, & &1.text) |> Enum.join()
      send(send_to, {:transcribed, transcription})
    end)
  end

  @impl true
  def handle_info({:transcribed, transcription}, _ctx, state) do
    IO.inspect(transcription, label: "Transcription")
    state = %{state | transcribing?: false}
    {[], state}
  end

  @impl true
  def handle_info(other_msg, _ctx, state) do
    Membrane.Logger.debug("Unknown message received: #{inspect(other_msg)}")
    {[], state}
  end
end
```

## Pipeline structure definition

The pipeline consists of three elements:

* The `Membrane.PortAudio.Source` - responsible for fetching the audio input from your microphone
* The `Membrane.FFmpeg.SWResampleConverter` - responsible for performing the sample rate conversion to the desired 16 kHz as well as the sample format conversion
* The `SpeechToText` sink we have previously created

```elixir
alias Membrane.FFmpeg

import Membrane.ChildrenSpec

spec =
  child(:microphone, Membrane.PortAudio.Source)
  |> child(:converter, %FFmpeg.SWResample.Converter{
    output_stream_format: %RawAudio{channels: 1, sample_format: :f32le, sample_rate: 16_000}
  })
  |> child(:speech_to_text, SpeechToText)

:ok
```

## Running of the pipeline

```elixir
alias Membrane.RCPipeline

pipeline = RCPipeline.start!()
```

Finally, we can commission `spec` action execution with the previously created `structure`:

```elixir
RCPipeline.exec_actions(pipeline, spec: spec)
```

The prints with the transcription should start appearing below the last cell you have evaluated.
Try saying something to the microphone in English and the transcription of your words should appear.

You can terminate the pipeline with the following code:

```elixir
RCPipeline.terminate(pipeline)
```
